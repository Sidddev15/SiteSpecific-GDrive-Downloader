# ğŸ“ Specific Web Document Downloader

A focused Python-based script built to scrape a **specific website**, extract **Google Drive links**, and automatically download those files into a local folder. It also logs downloaded links to avoid duplicates and skips inaccessible or broken links gracefully.

## ğŸš€ Features

- ğŸ”— Scrapes all specified URLs from a `links.txt` file.
- ğŸ§  Extracts only valid Google Drive links.
- ğŸ“¥ Downloads files to `website_docs/` folder.
- ğŸ” Skips files already downloaded (uses `downloaded_drive_links.txt`).
- âŒ Gracefully skips broken/inaccessible links.
- ğŸ§¾ Maintains logs for extracted and downloaded links.

## ğŸ§° Tech Stack

- Python 3.8+
- `requests`, `beautifulsoup4`, `gdown`
- Linux Shell compatible

## âš™ï¸ Setup & Usage
1. Clone the Repo
   ```bash
   git clone https://github.com/your-username/Specific_Web_Document_Downloader.git
   cd Specific_Web_Document_Downloader
   python3 -m venv venv
   source venv/bin/activate
   pip install -r requirements.txt
   python main.py
---

## ğŸ” Important Notes
- This script is custom-built for a specific site structure.
- It currently handles only Google Drive links.
- Make sure your environment has internet access and Python 3.8+.

---
## ğŸ› ï¸ Contributing
- Pull requests are welcome! If you'd like to add functionality, optimize performance, or improve documentation, feel free to fork and PR.
---
## Issues
- Please use the [issues](https://github.com/Sidddev15/SiteSpecific-GDrive-Downloader/issues) tab to report bugs or request features.
---
## ğŸ™Œ Acknowledgments
Built with â¤ï¸ by Siddharth Singh
Special thanks to gdown, beautifulsoup4, and Pythonâ€™s awesome community.
---
## ğŸ’¬ Let's Connect

Let me know if you want me to:

- Generate dynamic badges (e.g., `python version`, `last updated`, `stars`)
- Create a cover or diagram image
- Add license, `.gitignore`, and `requirements.txt` boilerplate

Just say the word and Iâ€™ll prep it all!
---
## ğŸ“‚ Folder Structure

```plaintext
Specific_Web_Document_Downloader/
â”œâ”€â”€ website_docs/                   # All downloaded files go here
â”œâ”€â”€ links.txt                       # List of URLs to scrape
â”œâ”€â”€ drive_links_extracted.txt       # All Drive links found
â”œâ”€â”€ downloaded_drive_links.txt      # Only downloaded Drive links
â”œâ”€â”€ main.py                         # Main executable script
â””â”€â”€ README.md
